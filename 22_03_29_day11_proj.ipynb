{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"22_03_29_day11_proj.ipynb","provenance":[],"authorship_tag":"ABX9TyMRj+cfQ3aloVLZhCS47sCm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# seq2seq+attn 스페인-영어 번역기"],"metadata":{"id":"40sFurxcDSZQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rS-4sIIlu5uv"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.ticker as ticker\n","import matplotlib.pyplot as plt\n","\n","import time\n","import re\n","import os\n","import io"]},{"cell_type":"markdown","source":["## 데이터 불러오기"],"metadata":{"id":"-lkXLoViDXO2"}},{"cell_type":"code","source":["path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMiUYOl9DMkK","executionInfo":{"status":"ok","timestamp":1648515922813,"user_tz":-540,"elapsed":28,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"914e2bd3-fe99-455e-cb2f-6558f5246975"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2646016/2638744 [==============================] - 0s 0us/step\n","2654208/2638744 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\""],"metadata":{"id":"4MWVJ2AcD2tZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(path_to_file, \"r\") as f:\n","    raw = f.read().splitlines()\n","\n","print(\"Data Size : \", len(raw))\n","print(\"Example : \")\n","\n","for sen in raw[0:100][::20]: print(\">>\", sen)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vqwfrRcESUS","executionInfo":{"status":"ok","timestamp":1648515923205,"user_tz":-540,"elapsed":9,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"fdba03e6-3e76-439e-dc60-125c146d38ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Size :  118964\n","Example : \n",">> Go.\tVe.\n",">> Wait.\tEsperen.\n",">> Hug me.\tAbrázame.\n",">> No way!\t¡Ni cagando!\n",">> Call me.\tLlamame.\n"]}]},{"cell_type":"markdown","source":["\\t "],"metadata":{"id":"C1MCl5QjFNHG"}},{"cell_type":"markdown","source":["## 데이터 전처리 : 정제하기"],"metadata":{"id":"0ufEZbc_E1bM"}},{"cell_type":"code","source":["def preprocess_sentence(sentence, s_token=False, e_token=False):\n","    sentence = sentence.lower().strip()\n","\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n","\n","    sentence = sentence.strip()\n","\n","    if s_token:\n","        sentence = '<start> ' + sentence\n","    \n","    if e_token:\n","        sentence += ' <end>'\n","\n","    return sentence"],"metadata":{"id":"PFkhxlhfEjQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_corpus = []\n","dec_corpus = []\n","\n","num_examples = 30000\n","\n","for pair in raw[:num_examples]:\n","    eng, spa = pair.split(\"\\t\")\n","\n","    enc_corpus.append(preprocess_sentence(eng))\n","    dec_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True))\n","\n","print(\"English :\", enc_corpus[100]) # 영어 텍스트\n","print(\"Spanish :\", dec_corpus[100]) # 스페인 텍스트"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FovuN_6rGRIJ","executionInfo":{"status":"ok","timestamp":1648515924877,"user_tz":-540,"elapsed":1678,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"80618ac0-64a2-401a-d6e9-946400c2fc17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English : go away !\n","Spanish : <start> salga de aqu ! <end>\n"]}]},{"cell_type":"markdown","source":["## 데이터 전처리 : 토큰화"],"metadata":{"id":"B-SmQT17IDxF"}},{"cell_type":"code","source":["def tokenize(corpus):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","    tokenizer.fit_on_texts(corpus)\n","\n","    tensor = tokenizer.texts_to_sequences(corpus)\n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n","\n","    return tensor, tokenizer"],"metadata":{"id":"5bdYPgctHnN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰화\n","enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n","dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n","\n","# 훈련 데이터와 검증 데이터로 분리하기 (80/20) ---> test_size = 0.2\n","enc_train, enc_val, dec_train, dec_val = train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n","\n","print(\"English Vocab Size : \", len(enc_tokenizer.index_word))\n","print(\"Spanish Vocab Size : \", len(dec_tokenizer.index_word))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlyHzTJ4Irva","executionInfo":{"status":"ok","timestamp":1648516528968,"user_tz":-540,"elapsed":1178,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"c54f9acf-eb54-462a-9f3d-46e4a2db13d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English Vocab Size :  4931\n","Spanish Vocab Size :  8893\n"]}]},{"cell_type":"markdown","source":["# 모델 설계"],"metadata":{"id":"FabvTU3CPsMV"}},{"cell_type":"markdown","source":["![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-P-2.max-800x600.jpg)"],"metadata":{"id":"Dh1pnGf1Py-X"}},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.w_decoder = tf.keras.layers.Dense(units)\n","        self.w_encoder = tf.keras.layers.Dense(units)\n","        self.w_combine = tf.keras.layers.Dense(1)\n","    \n","    def call(self, h_encoder, h_decoder):\n","        # h_enc shape : [batch x length x units]\n","        # h_dec shape : [batch x units]\n","\n","        h_encoder = self.w_encoder(h_encoder)\n","\n","        h_decoder = tf.expand_dims(h_decoder, 1)\n","        h_decoder = self.w_decoder(h_decoder)\n","\n","        score = self.w_combine(tf.nn.tanh(h_decoder + h_encoder))\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        context_vector = attention_weights * h_decoder\n","        context_vector = tf.reduce_sum(context_vector, axis = 1)\n","\n","        return context_vector, attention_weights"],"metadata":{"id":"9KKZj3pXPdYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units):\n","        super(Encoder, self).__init__()\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True)\n","\n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.gru(out)\n","\n","        return out"],"metadata":{"id":"gqtwCJm_Qyq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units):\n","        super(Decoder, self).__init__()\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state=True)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, h_dec, enc_out):\n","        context_vec, attn = self.attention(enc_out, h_dec)\n","        #print(\"컨텍스트 벡터의 차원 : \", context_vec.shape) #(64, 1024)\n","    \n","        out = self.embedding(x)\n","        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis= -1)\n","\n","        out, h_dec = self.gru(out)\n","        #print(\"gru 차원 : \", out.shape) #(64, 1, 1024)\n","        \n","        out = tf.reshape(out, (-1, out.shape[2])) # (64, 1024)\n","        #print(\"변경된 out의 차원 : \", out.shape)\n","        out = self.fc(out) \n","        #print(\"fc 거친 out의 차원 :\", out.shape) # (64, 8894)\n","\n","        return out, h_dec, attn"],"metadata":{"id":"DiJ6zBQHRgF2","executionInfo":{"status":"ok","timestamp":1648521165178,"user_tz":-540,"elapsed":252,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","src_vocab_size = len(enc_tokenizer.index_word) + 1\n","tgt_vocab_size = len(dec_tokenizer.index_word) + 1\n","\n","units = 1024\n","embedding_dim = 512\n","\n","encoder = Encoder(src_vocab_size, embedding_dim, units)\n","decoder = Decoder(tgt_vocab_size, embedding_dim, units)\n","\n","######\n","# sample input\n","sequence_len = 30\n","\n","sample_enc = tf.random.uniform((batch_size, sequence_len))\n","sample_output = encoder(sample_enc)\n","\n","print('Encoder Output: ', sample_output.shape)\n","\n","sample_state = tf.random.uniform((batch_size, units))\n","\n","sample_logits, h_dec, attn = decoder(tf.random.uniform((batch_size, 1)), sample_state, sample_output)\n","\n","print('Decoder output :', sample_logits.shape)\n","print('Decoder Hidden state :', h_dec.shape)\n","print('Attention :', attn.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QjqNifrRzl6","executionInfo":{"status":"ok","timestamp":1648521168864,"user_tz":-540,"elapsed":3457,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"903dd894-7673-494e-d9fc-5ec557fbd490"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder Output:  (64, 30, 1024)\n","Decoder output : (64, 8894)\n","Decoder Hidden state : (64, 1024)\n","Attention : (64, 30, 1)\n"]}]},{"cell_type":"markdown","source":["## 훈련하기 : Optimizer & loss"],"metadata":{"id":"1l_xklh7hm5h"}},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') # 모델 출력값을 그대로 전달한다.\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real,0))\n","    loss = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss.dtype) # <PAD>\n","    loss *= mask\n","\n","    return tf.reduce_mean(loss)\n","\n","    #[ 0.1 0.2 0.7] ----> 2"],"metadata":{"id":"ouE18a4NS069","executionInfo":{"status":"ok","timestamp":1648521494931,"user_tz":-540,"elapsed":240,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["## 훈련하기 : train_step 구하기"],"metadata":{"id":"IGVXcL1ujViV"}},{"cell_type":"markdown","source":["train step 학습과정\n","1. Encoder에 소스 문장을 전달해 컨텍스트 벡터인 enc_out을 생성\n","2. Decoder에 입력으로 전달할 토큰 문장 생성\n","3. t=0일 때, Decoder의 Hidden state는 Encoder의 Final state로 정의. h_dec = enc_out[:, -1]\n","4. 문장과 enc_out, Hidden state를 기반으로 다음단어 (t=1)예측 pred\n","5. 예측된 단어와 정답간의 loss을 구한 후, t=1의 정답 단어를 다음 입력으로 사용 (예측단어X)\n","6. 반복!"],"metadata":{"id":"ZdzY6ZK4k5yi"}},{"cell_type":"code","source":["@tf.function # 가속 연산\n","def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n","    bsz = src.shape[0]\n","    loss = 0\n","\n","    with tf.GradientTape() as tape: # 학습하면서 발생한 모든 연산을 기록하는 테이프\n","        enc_out = encoder(src)\n","        h_dec = enc_out[:, -1]\n","        \n","        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n","\n","        for t in range(1, tgt.shape[1]):\n","            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n","\n","            loss += loss_function(tgt[:, t], pred)\n","            dec_src = tf.expand_dims(tgt[:, t], 1)\n","        \n","    batch_loss = (loss / int(tgt.shape[1]))\n","\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    \n","    return batch_loss"],"metadata":{"id":"WXUPno9Vib5A","executionInfo":{"status":"ok","timestamp":1648521793148,"user_tz":-540,"elapsed":5,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["## 훈련하기 : 훈련시작하기"],"metadata":{"id":"0nPcMVv7oD4i"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import random\n","\n","epochs = 10\n","\n","for epoch in range(epochs):\n","    total_loss = 0\n","\n","    idx_list = list(range(0, enc_train.shape[0], batch_size))\n","    random.shuffle(idx_list)\n","    t = tqdm(idx_list)\n","\n","    for (batch, idx) in enumerate(t):\n","        batch_loss = train_step(enc_train[idx:idx+batch_size],\n","                                dec_train[idx:idx+batch_size],\n","                                encoder,\n","                                decoder,\n","                                optimizer,\n","                                dec_tokenizer)\n","        total_loss += batch_loss\n","\n","        t.set_description_str('Epoch %2d' % (epoch + 1))\n","        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqnKgVyojkuY","executionInfo":{"status":"ok","timestamp":1648524406417,"user_tz":-540,"elapsed":1088253,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"d7b39d5c-dfa7-4f3a-b31a-6759659f7629"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch  1: 100%|██████████| 375/375 [02:17<00:00,  2.72it/s, Loss 1.5000]\n","Epoch  2: 100%|██████████| 375/375 [01:46<00:00,  3.53it/s, Loss 1.2047]\n","Epoch  3: 100%|██████████| 375/375 [01:45<00:00,  3.55it/s, Loss 1.0403]\n","Epoch  4: 100%|██████████| 375/375 [01:45<00:00,  3.55it/s, Loss 0.9297]\n","Epoch  5: 100%|██████████| 375/375 [01:45<00:00,  3.56it/s, Loss 0.8520]\n","Epoch  6: 100%|██████████| 375/375 [01:46<00:00,  3.53it/s, Loss 0.7959]\n","Epoch  7: 100%|██████████| 375/375 [01:45<00:00,  3.56it/s, Loss 0.7480]\n","Epoch  8: 100%|██████████| 375/375 [01:45<00:00,  3.56it/s, Loss 0.7112]\n","Epoch  9: 100%|██████████| 375/375 [01:45<00:00,  3.56it/s, Loss 0.6801]\n","Epoch 10: 100%|██████████| 375/375 [01:45<00:00,  3.57it/s, Loss 0.6608]\n"]}]},{"cell_type":"code","source":["@tf.function\n","def eval_step(src, tgt, encoder, decoder, dec_tok):\n","  bsz = src.shape[0]\n","  loss =0\n","\n","  enc_out = encoder(src)\n","  h_dec = enc_out[:, -1]\n","\n","  dec_src = tf.expand_dims([dec_tok.word_index['<start>']]* bsz, 1)\n","\n","  for t in range(1, tgt.shape[1]):\n","    pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n","    loss += loss_function(tgt[:, t], pred)\n","    dec_src = tf.expand_dims(tgt[:, t], 1)\n","\n","  batch_loss = (loss/int(tgt.shape[1]))\n","\n","  return batch_loss"],"metadata":{"id":"aqng7Z6PpZFT","executionInfo":{"status":"ok","timestamp":1648524406418,"user_tz":-540,"elapsed":15,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Training Process\n","from tqdm import tqdm\n","\n","EPOCHS = 1\n","\n","for epoch in range(EPOCHS):\n","  total_loss = 0\n","\n","  idx_list = list(range(0, enc_train.shape[0], batch_size))\n","  random.shuffle(idx_list)\n","  t = tqdm(idx_list)\n","\n","  for (batch, idx) in enumerate(t):\n","    batch_loss = train_step(enc_train[idx:idx+batch_size],\n","                            dec_train[idx:idx+batch_size],\n","                            encoder,\n","                            decoder,\n","                            optimizer,\n","                            dec_tokenizer)\n","    total_loss += batch_loss\n","\n","  t.set_description_str('Epoch %2d' % (epoch +1))\n","  t.set_postfix_str('Loss %.4f' % (total_loss.numpy()/ (batch+1)))\n","\n","  test_loss = 0\n","\n","  idx_list = list(range(0, enc_val.shape[0], batch_size))\n","  random.shuffle(idx_list)\n","  t = tqdm(idx_list)\n","\n","  for (test_batch, idx) in enumerate(t):\n","    test_batch_loss = eval_step(enc_val[idx:idx+batch_size],\n","                                dec_val[idx:idx+batch_size],\n","                                encoder,\n","                                decoder,\n","                                dec_tokenizer)\n","    test_loss += test_batch_loss\n","\n","  t.set_description_str('Test Epoch %2d' % (epoch+1))\n","  t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy()/ (test_batch+1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-JdGnzPqNnq","executionInfo":{"status":"ok","timestamp":1648524795016,"user_tz":-540,"elapsed":129113,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"3413cff8-65a2-48c2-d845-7b6858905645"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 375/375 [01:44<00:00,  3.60it/s]\n","100%|██████████| 94/94 [00:24<00:00,  3.88it/s]\n"]}]},{"cell_type":"code","source":["def evaluate(sentence, encoder, decoder):\n","  attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n","  sentence = preprocess_sentence(sentence)\n","  inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n","                                                         maxlen = enc_train.shape[-1],\n","                                                         padding= 'post')\n","  \n","  result = ''\n","  enc_out = encoder(inputs)\n","  dec_hidden = enc_out[:, -1]\n","  dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n","\n","  for t in range(dec_train.shape[-1]):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","    \n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention[t] = attention_weights.numpy()\n","\n","    predicted_id = \\\n","    tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n","\n","    result += dec_tokenizer.index_word[predicted_id] + ' '\n","\n","    if dec_tokenizer.index_word[predicted_id] == '<end>':\n","      return result, sentence, attention\n","\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence, attention"],"metadata":{"id":"-juyZlM7p-1t","executionInfo":{"status":"ok","timestamp":1648525151208,"user_tz":-540,"elapsed":287,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def plot_attention(attention, sentence, predicted_sentence):\n","  fig = plt.figure(figsize=(10, 10))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention, cmap='viridis')\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + predicted_sentence, fontdict = fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()"],"metadata":{"id":"eGojJ0gwqOMP","executionInfo":{"status":"ok","timestamp":1648525152624,"user_tz":-540,"elapsed":3,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def translate(sentence, encoder, decoder):\n","  result, sentence, attention = evaluate(sentence, encoder, decoder)\n","\n","  print('Input : %s' % (sentence))\n","  print('Predicted translation : {}'.format(result))\n","\n","  attention = attention[:len(result.split()), :len(sentence.split())]\n","  plot_attention(attention, sentence.split(), result.split(' '))"],"metadata":{"id":"1AC_E1ZdqhcO","executionInfo":{"status":"ok","timestamp":1648525154410,"user_tz":-540,"elapsed":4,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["translate(\"Can I have some coffee?\", encoder, decoder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"_az5cDheqj_Z","executionInfo":{"status":"error","timestamp":1648530681989,"user_tz":-540,"elapsed":244,"user":{"displayName":"백혜림","userId":"01884416935170274848"}},"outputId":"60fe9aa5-3ac3-41fc-cd08-952094052d57"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fd0ee9842f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can I have some coffee?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'translate' is not defined"]}]},{"cell_type":"markdown","source":["# Proj. 한-영 번역기 만들기"],"metadata":{"id":"Sbx1EO-U_-iE"}},{"cell_type":"markdown","source":["한-영 번역기 만들기\n","1. 데이터 다운로드\n","- 데이터 : https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1\n","- korean-english-park.train.tar.gz\n","2. 데이터 정제\n","- set 데이터형이 중복이 허용하지 않다는 것을 활용해 중복된 데이터를 제거\n","  - 데이터 병렬 쌍이 흐트러지지 않게 주의!\n","  - cleaned_corpus에 저장\n","- 앞서 정의한 preprocessing()함수는 한글에 대해 동작하지 않아요.\n","  - 한글에 적용할 수 있는 정규식을 추가해여 함수를 재정의 하세요.\n","- 타겟 언어인 영문엔 <\\start>토큰과 <\\end>토큰을 추가하고 split()함수로 토큰화 합니다. 한글 토큰화는 konlpy의 mecab클래스를 사용합니다.\n","  - cleaned_corpus로부터 토큰의길이가 40이하인 데이터를 선별하여 eng_corpus와 kor_corpus를 각각 구축하기\n","\n","3. 토큰화\n","- tokenize()함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요!\n","  - 단어수는 실험을 통해 적당한 값을 맞춰줍시다(최소 10000이상!)\n","4. 훈련하기"],"metadata":{"id":"UcDCK2QRABWs"}},{"cell_type":"code","source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n"," \n","%config InlineBackend.figure_format = 'retina'\n"," \n","import matplotlib.font_manager as fm\n","fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n","font = fm.FontProperties(fname=fontpath, size=9)\n","plt.rc('font', family='NanumBarunGothic') \n","mpl.font_manager._rebuild()"],"metadata":{"id":"NxzaKKKAql40","executionInfo":{"status":"ok","timestamp":1648529277661,"user_tz":-540,"elapsed":1104,"user":{"displayName":"백혜림","userId":"01884416935170274848"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"gg0Pd9SyAGZq"},"execution_count":null,"outputs":[]}]}